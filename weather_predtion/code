#!/usr/bin/env python
# encoding=utf8
import urllib
import re
import json
import urllib, urllib2
import unicodedata
import sys, getopt
import os
import urlparse
import HTMLParser
from bs4 import BeautifulSoup
#from BeautifulSoup import BeautifulSoup
import codecs
#import nltk
import requests, base64
import sys
reload(sys)
sys.setdefaultencoding('utf8')
class cleanUp:
        NEGATIVE    = re.compile("comment|meta|footer|footnote|foot")
        POSITIVE    = re.compile("post|hentry|entry|content|text|body|article|table")
        PUNCTUATION = re.compile("""[!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]""")


        def remove_non_ascii(self, text):
              return ''.join([i if ord(i) < 128 else ' ' for i in text])

        def grabContent(self,link, html):
                #print "########## inside grabContent #########"
                replaceBrs = re.compile("<br */? *>[ \r\n]*<br */? *>")
                html = re.sub(replaceBrs, "</p><p>", html)
                try:
                        soup = BeautifulSoup(html)
                except HTMLParser.HTMLParseError:
                        return ""

    # REMOVE SCRIPTS
                for s in soup.findAll("script"):
                        s.extract()

                #allParagraphs = soup.findAll("p")
                allParagraphs = soup.findAll(['p','table'])
                #print type(allParagraphs)
                #append.allParagraphs(soup.findAll('table'))
                topParent     = None
                #table = soup.findAll("table")
                #print table

                #print allParagraphs
                parents = []
                for paragraph in allParagraphs:
                        parent = paragraph.parent
                        #print dir(parent)
                        if (parent not in parents):
                                parents.append(parent)
                                parent.score = 0

                                if (parent.has_attr("class")):
                                    if type(parent["class"]).__name__=='list':
                                        parent_class = ''.join(map(str,parent["class"]))
                                        if (self.NEGATIVE.match(parent_class)):
                                                parent.score -= 50
                                        if (self.POSITIVE.match(parent_class)):
                                                parent.score += 25
                                    else:
                                        if (self.NEGATIVE.match(parent["class"])):
                                                parent.score -= 50
                                        if (self.POSITIVE.match(parent["class"])):
                                                parent.score += 25

                                if (parent.has_attr("id")):
                                        if (self.NEGATIVE.match(parent["id"])):
                                                parent.score -= 50
                                        if (self.POSITIVE.match(parent["id"])):
                                                parent.score += 25


                        if (parent.score == None):
                                parent.score = 0

                        innerText = paragraph.renderContents() #"".join(paragraph.findAll(text=True))
                        if (len(innerText) > 10):
                                parent.score += 1

                        parent.score += innerText.count(",")

                for parent in parents:
                        if ((not topParent) or (parent.score > topParent.score)):
                                topParent = parent

                if (not topParent):
                        return ""

                # REMOVE LINK'D STYLES
                styleLinks = soup.findAll("link", attrs={"type" : "text/css"})
                for s in styleLinks:
                        s.extract()

                # REMOVE ON PAGE STYLES
                for s in soup.findAll("style"):
                        s.extract()

                # CLEAN STYLES FROM ELEMENTS IN TOP PARENT
                for ele in topParent.findAll(True):
                        del(ele['style'])
                        del(ele['class'])

                self.killDivs(topParent)
                self.clean(topParent, "form")
                self.clean(topParent, "object")
                self.clean(topParent, "iframe")

                #fixLinks(topParent, link)
                #print topParent.renderContents()
                #print "########## exiting grabContent #########"
                #return topParent.renderContents()
                return  self.remove_non_ascii(BeautifulSoup(topParent.renderContents()).get_text().replace("\t", "").replace("\r", "").replace("\n", "")).decode('unicode_escape').encode('ascii','ignore')
                #return BeautifulSoup(topParent.renderContents()).get_text()



        def clean(self,top, tag, minWords=1000):
                tags = top.findAll(tag)

                for t in tags:
                        if (t.renderContents().count(" ") < minWords):
                                t.extract()


        def killDivs(self,parent):

                divs = parent.findAll("div")
                for d in divs:
                        p     = len(d.findAll("p"))
                        img   = len(d.findAll("img"))
                        li    = len(d.findAll("li"))
                        a     = len(d.findAll("a"))
                        embed = len(d.findAll("embed"))
                        pre   = len(d.findAll("pre"))
                        code  = len(d.findAll("code"))

                        if (d.renderContents().count(",") < 10):
                                if ((pre == 0) and (code == 0)):
                                        if ((img > p ) or (li > p) or (a > p) or (p == 0) or (embed > 0)):
                                                d.extract()


        def cleanData(self,url):
                #url  = "http://advanceindiana.blogspot.com/2014/01/did-marions-mayor-wayne-seybold-gamble.html"
                print "########## inside clean Data ##########"
                try:
                     response = urllib2.urlopen(url, timeout=150)
                     print "********************",response
                except urllib2.URLError:
                     print "Bad URL or timeout"
                #response  = urllib2.urlopen(url)
                print 'got url response'
                html = response.read()
                html.decode('utf-8','ignore')
                html1 = "".join(i for i in html if ord(i)<128)
                #print html1
                contents_details = self.grabContent("", html1)
                print "########## exiting cleanData #########k"
                #return BeautifulSoup(contents_details).get_text().replace("\t", "").replace("\r", "").replace("\n", "")
                #return (BeautifulSoup(contents_details).get_text()).encode('ascii', 'ignore').decode('ascii')
                return self.remove_non_ascii(BeautifulSoup(contents_details).get_text()).decode('unicode_escape').encode('ascii','ignore')
                #return nltk.clean_html(contents_details)

if __name__ == "__main__":
                        cleanup = cleanUp()
                        url  = "http://www.biography.com/people/narendra-modi"
                        url = "https://www.online.citibank.co.in/customerservice/cs-faq.htm"
                        url = "http://socialize.morningstar.com/NewSocialize/forums/p/359192/3732397.aspx#3732397"
                        url = "http://www.cafepharma.com/boards/threads/be-a-proud-american-and-show-your-support.594128/"
                        url = "http://socialize.morningstar.com/NewSocialize/forums/p/359158/3732358.aspx#3732358"
                        url = "http://lifestyle.simplymovein.com/2016/04/14/top-residential-locations-in-bangalore/"
                        url = "http://www.ifrs.org/Rate-regulated-activities/Exposure-draft-and-Comment-Letters/Comment-Letters/Pages/Comment-Letters.aspx"
                        url = "https://en.wikipedia.org/wiki/Apache_Spark"
                        #url = sys.argv[1]
                        #print url
                        #url = "http://www.bookcomplaints.com/online-consumer-complaint/India/naukri-com-reviews-complaints/job-career/online-job-portal/naukri-com-unneccessary-mails-sent_i20963#.VyB4l_l97IU&gsc.tab=0"
                        #url = "http://timesofindia.indiatimes.com/india/PM-Modi-puts-his-stamp-on-rules-for-governments-allocation-and-transaction-business/articleshow/47537499.cms"
                        #url = "http://www.cfo.com/printable/article.cfm/3860276?f=options&utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+genpact%2FZRgq+%28Genpact+News%29"
                        #url = "http://www.independent.co.uk/news/uk/politics/eu-referendum-david-camerons-rules-are-a-democratic-disgrace-says-frenchborn-scottish-politician-set-to-be-denied-a-vote-10274442.html"
                        raw = cleanup.cleanData(url)
                        print raw
                        #print raw.rstrip('\r\n').replace("\n", " ").replace("\t", " ")
                        #print ''.join(raw.splitlines())
                        #print raw.rstrip('\r\n')
